service: escenic-cue-live-api

variablesResolutionMode: 20210326
configValidationMode: error

plugins:
  - serverless-plugin-aws-alerts
  - serverless-dynamodb-local
  - serverless-plugin-typescript
  - serverless-offline #serverless-offline needs to be last in the list

custom:
  env: ${opt:stage,'stg'}
  proj: ${opt:proj,'ece'} # !TODO
  prefix: ${self:custom.env}-${self:custom.proj}-${self:service}
  queueName: ${self:custom.prefix}
  dynamodbTableName: ${self:custom.prefix}-entries-table
 
  dynamoTableName:
    ${self:custom.dynamodbTableName}
    # domains
  dynamodb:
    # If you only want to use DynamoDB Local in some stages, declare them here
    stages:
      - local
    start:
      port: 8000
      inMemory: true
      heapInitial: 200m
      heapMax: 1g
      migrate: true
      convertEmptyValues: true
    migration:
      dir: dynamodbMigrations,
      table_prefix: ''
      table_suffix: ''
    seed:
      test:
        sources:
          - table: ${self:custom.dynamodbTableName}
            sources: [./dynamo-seed.json]

  alerts:
    stages: # Optionally - select which stages to deploy alarms to
      - prod
      - stg
    definitions:
      functionErrors:
        threshold: 10
        treatMissingData: notBreaching # override treatMissingData
      functionInvocations:
        threshold: 30
        evaluationPeriods: 5
        datapointsToAlarm: 4
        treatMissingData: notBreaching # override treatMissingData
      functionDuration:
        threshold: 180000
        treatMissingData: notBreaching # override treatMissingData
      functionThrottles:
        threshold: 3
        evaluationPeriods: 2
        datapointsToAlarm: 2
        treatMissingData: notBreaching # override treatMissingData
    topics:
      alarm:
        topic: ${cf:${self:custom.env}-${self:custom.proj}-base-sns.InfrastructureMonitoringTopic}
    function:
      - functionErrors
      - functionInvocations
      - functionDuration
      - functionThrottles
      - name: logErrorAlarm
        metric: ErrorLogs
        statistic: Sum
        period: 60 # in seconds
        evaluationPeriods: 1
        comparisonOperator: GreaterThanOrEqualToThreshold
        threshold: 10
        treatMissingData: notBreaching
        pattern: '{$.level>=50}'

provider:
  name: aws
  runtime: nodejs18.x
  stage: ${opt:stage, 'stg'} # Set the default stage used. Default is dev
  region: eu-central-1 # Overwrite the default region used. Default is us-east-1
  memorySize: 128 # Overwrite the default memory size. Default is 1024
  timeout: 5 # The default is 6 seconds. Note: API Gateway current maximum is 30 seconds
  logRetentionInDays: 7 # Set the default RetentionInDays for a CloudWatch LogGroup
  stackName: ${self:custom.prefix}
  versionFunctions: false # disabled versioning mechanism
  tracing:
    lambda: true
    apiGateway: true
  logs:
    restApi: false
  apiGateway:
    metrics: true
    apiKeys:
      - ${opt:stage}-internal
  iam:
    role:
      statements:
        - Effect: Allow
          Action:
            - dynamodb:Query
            - dynamodb:Scan
            - dynamodb:GetItem
            - dynamodb:PutItem
          Resource: '*'
        - Effect: Allow
          Action:
            - sqs:SendMessage
            - sqs:SendMessageBatch
          Resource:
            - !GetAtt Queue.Arn
        - Effect: Allow
          Action:
            - xray:PutTraceSegment
            - xray:PutTraceSegments
            - xray:PutTelemetryRecords
          Resource: '*'

  # you can define service wide environment variables here
  environment:
      STAGE: ${self:provider.stage}
      QUEUE_HOST: 
          Ref: Queue
      BLICK_ESCENIC_CUE_LIVE_ENTRIES_TABLE_NAME: ${self:custom.dynamodbTableName}


# you can add packaging information here
package:
  individually: true
  patterns:
    - '!README.md'
    - '!test/**'

functions:
  listener:
    name: ${self:custom.prefix}-listener
    description: 'The lambda function provides endpoints for saving CUE Live event entries updates to SQS'
    handler: src/handlers/listener.handler
    memorySize: 256 # Overwrite the default memory size. Default is 1024
    timeout: 30 # The default is 6 seconds. Note: API Gateway current maximum is 30 seconds
    tags: # Function specific tags
      Environment: ${self:provider.stage}
      Name: ${self:custom.prefix}-listener
      Project: ${self:custom.proj}
    environment:

      NODE_ENV: '${self:provider.stage}'
      LOG_LEVEL: debug
    events:
      - http:
          path: '/entry'
          method: post
          private: true
  consumer:
    name: ${self:custom.prefix}-consumer
    description: 'The lambda function consumes SQS and transforms CUE Live entries by calling Cook'
    handler: src/handlers/consumer.handler
    timeout: 5 # Timeout for this specific function.  Overrides the default set above.
    tags: # Function specific tags
      Environment: ${self:provider.stage}
      Name: ${self:custom.prefix}-consumer
      Project: ${self:custom.proj} 
    environment:
      STAGE: ${self:provider.stage}
      BLICK_COOK_ENDPOINT: ${ssm:/escenic-cue-live-api/cook/host}

    events:
      - sqs:
          arn: !GetAtt Queue.Arn
          batchSize: 1 # Use 1 to simplify how we handle the errors
  # thumbnail:
  #   name: ${self:custom.prefix}-blicktv-chapters-image-upload-lambda
  #   description: 'The lambda function is triggered by S3 and uploads the chapters thumbnails to NetStorage'
  #   handler: producer.lambdaImageHandler
  #   memorySize: 128 # Overwrite the default memory size. Default is 1024
  #   timeout: 180 # The default is 6 seconds. Note: API Gateway current maximum is 30 seconds
  #   tags: # Function specific tags
  #     Environment: ${self:provider.stage}
  #     Name: ${self:custom.prefix}-blicktv-chapters-image-upload-lambda
  #     Project: ${self:custom.proj}
  #   environment:
  #     STAGE: ${self:provider.stage}
  #     NODE_ENV: '${self:provider.stage}'
  #     LOG_LEVEL: debug
  #     BLICK_CHAPTERS_TABLE_NAME: ${self:custom.dynamoTableName}
  #     BLICK_CHAPTERS_BUCKET_NAME: ${self:custom.bucketName}
  #     BLICK_CHAPTERS_AKAMAI_NETSTORAGE_HOST: ${ssm:/chapters-lambda/akamai/netstorage/host}
  #     BLICK_CHAPTERS_AKAMAI_NETSTORAGE_USER: ${ssm:/chapters-lambda/akamai/netstorage/user}
  #     BLICK_CHAPTERS_AKAMAI_NETSTORAGE_KEY: ${ssm:/chapters-lambda/akamai/netstorage/key}
  #     BLICK_CHAPTERS_AKAMAI_NETSTORAGE_CP_CODE: ${ssm:/chapters-lambda/akamai/netstorage/cpCode}

resources:
  Resources:
    Queue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:custom.queueName}
        VisibilityTimeout: 5
        RedrivePolicy:
          deadLetterTargetArn:
            Fn::GetAtt:
              - 'DeadLetterQueue'
              - 'Arn'
          maxReceiveCount: 5
        Tags:
          - Key: Environment
            Value: ${self:custom.env}
          - Key: Name
            Value: ${self:custom.queueName}
          - Key: Project
            Value: ${self:custom.prefix}

    DeadLetterQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:custom.queueName}-dlq
        MessageRetentionPeriod: 1209600 # 14 days
        VisibilityTimeout: 5
    # DynamoDBTable:
    #   Type: 'AWS::DynamoDB::Table'
    #   DeletionPolicy: Retain
    #   Properties:
    #     AttributeDefinitions:
    #       - AttributeName: 'type'
    #         AttributeType: 'S'
    #       - AttributeName: 'full_timestamp'
    #         AttributeType: 'S'
    #     KeySchema:
    #       - AttributeName: 'type'
    #         KeyType: 'HASH'
    #       - AttributeName: 'full_timestamp'
    #         KeyType: 'RANGE'
    #     ProvisionedThroughput:
    #       ReadCapacityUnits: '5'
    #       WriteCapacityUnits: '1'
    #     TableName: ${self:custom.dynamoTableName}
    #     TimeToLiveSpecification:
    #       AttributeName: 'expire_at'
    #       Enabled: true
    #     Tags: # Function specific tags
    #       - Key: Environment
    #         Value: ${self:custom.env}
    #       - Key: Name
    #         Value: ${self:custom.prefix}-${self:service}
    #       - Key: Project
    #         Value: ${self:custom.proj}
    # ScalingRole:
    #   Type: AWS::IAM::Role
    #   Properties:
    #     AssumeRolePolicyDocument:
    #       Version: '2012-10-17'
    #       Statement:
    #         - Effect: 'Allow'
    #           Principal:
    #             Service:
    #               - application-autoscaling.amazonaws.com
    #           Action:
    #             - 'sts:AssumeRole'
    #     Path: '/'
    #     Policies:
    #       - PolicyName: ScalingIAMRolePolicy
    #         PolicyDocument:
    #           Version: '2012-10-17'
    #           Statement:
    #             - Effect: 'Allow'
    #               Action:
    #                 - 'dynamodb:DescribeTable'
    #                 - 'dynamodb:UpdateTable'
    #                 - 'cloudwatch:PutMetricAlarm'
    #                 - 'cloudwatch:DescribeAlarms'
    #                 - 'cloudwatch:GetMetricStatistics'
    #                 - 'cloudwatch:SetAlarmState'
    #                 - 'cloudwatch:DeleteAlarms'
    #               Resource: '*'
    # ReadScalingPolicy:
    #   Type: AWS::ApplicationAutoScaling::ScalingPolicy
    #   Properties:
    #     PolicyName: DynamoReadAutoScalingPolicy
    #     PolicyType: TargetTrackingScaling
    #     ScalingTargetId: !Ref ReadCapacityScalableTarget
    #     TargetTrackingScalingPolicyConfiguration:
    #       TargetValue: 90
    #       ScaleInCooldown: 60
    #       ScaleOutCooldown: 60
    #       PredefinedMetricSpecification:
    #         PredefinedMetricType: DynamoDBReadCapacityUtilization
    # ReadCapacityScalableTarget:
    #   Type: AWS::ApplicationAutoScaling::ScalableTarget
    #   Properties:
    #     MaxCapacity: 140
    #     MinCapacity: 5
    #     ResourceId: !Join
    #       - /
    #       - - table
    #         - !Ref DynamoDBTable
    #     RoleARN: !GetAtt ScalingRole.Arn
    #     ScalableDimension: dynamodb:table:ReadCapacityUnits
    #     ServiceNamespace: dynamodb
